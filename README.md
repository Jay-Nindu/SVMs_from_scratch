# SVMs-from-scratch
Support vector machines are a staple of ML classification algorithms. This project aims to implement SVMs from scratch, in c++, and to detail the theory behind them. The optimisation algorithm used for my SVM is sequential minimal optimisation (SMO) as outlined in [2] (refereneces at bottom of document). The theoretical background for this project can be found in [1].
I reuse the matrix class that I implemented for my neural networks in c++ project (https://github.com/Jay-Nindu/neural_net_in_cxx).

## Comparing my support vector machine to sci-kit-learn
I tested my custom SVM classifier by comparing the decision boundaries it generated with the decision boundaries generated by the SVM implemented in the scikitlearn library. Both classifiers were given a dataset consisting of features extracted from iris flowers (sepal width and sepal length), and was tasked with identifying if a flower was of the versicolor species or not. The below graph visualises this.
![](images/sepal_features.png)

The sci-kit learn SVM (see source code in file "scikitlearn.py") was able to classify 72.5% of flowers correctly using a linear kernel. This increased to 80.83% when using an RBF kernel. See these graphs below demonstrating the decision boundaries:
![](images/skl_linear.png)
![](images/skl_rbf.png)

My custom SVM achieved 65% accuracy with a linear kernel and 70.833% with an RBF kernel. See the below generated graphs (generated in python using the outputs from my c++ program, see "linearOutput.txt" and "rbfOutput.txt" for the original outputs) for the decision boundaries. 

![](images/my_linear.png)
![](images/my_rbf.png)

Ultimately, not a bad attempt at recreating the SVM. However, there are some improvement that could be made. 

## The theory (ignore for now - writeup in process)
A support vector machine aims to create a hyperplane (defined b the equation $wx + b = 0$) that separates two sets of data into two labels (+1 or -1) with as large a margin as possible between the two data sets. We normalise the hyperplane based on the closest points of the hyperplane, so that each of $|wx+b| = 1$ In summary:
- $y_i = +1 \implies wx + b > 1$
- similarly $y_i = -1 \implies wx + b < -1$.

These equations can be combined as: $y_i(wx_i+b) > 1, \forall i$. Or for ease in future equations: $y_i(wx_i+b) - 1 > 0, \forall i$

We maximise the distance of our support vectors (points closest to the hyperplane) for both classess. We can calculate this distance (euclidean norm) of the closest data point $x_n$ to the hyperplanem by taking any point on the hyperplane $x$ and projecting the vector between these points ($x_n - x$) onto a vector orthogonal with our hyperplane (i.e. the normalised vector of $w$ which defines our hyperplane). This reduces down to: $\text{Distance} = \frac{1}{|w|} \cdot |1 + 0| = \frac{1}{|w|}$ We must maximise the above margin, which is dependant on minimising $|w| = w^{T}w$.

Thus we must solve the following eqn:
```math
\begin{equation}
\min_{w, b \text{ subject to } y_i(wx_i+b) -1 \geq 0 \text{, } \forall i} \frac{1}{2} w^{T}w
\end{equation}
```
Note: I have introduced $\frac{1}{2}$ for ease later on, it does not change the objective function.

To deal with the constraints upon this optimisation, we directly include the constraints in our equation with lagrangian multipliers. 
```math
\begin{equation}
\min_{w,b \text{ subject to } \alpha_i \geq 0 \text{, } \forall i} \frac{1}{2} w^{T}w - \sum_{i=1}^N \langle \alpha_i,  y_i(wx_i + b - 1) \rangle
\end{equation}
```


Now, to solve this equation. Setting the derivative wrt w to 0 results in the constraint:
```math
\begin{equation}
w - \sum_{i=1}^N \langle \alpha_i, y_{i}x_i \rangle = 0
\end{equation}
```

Setting the derivative wrt b to 0 results in the constraint:
```math
\begin{equation}
\sum_{i=1}^N \langle \alpha_i, y_{i} \rangle = 0
\end{equation}
```

Note that the inner product $\langle a, b \rangle$ for the linear case is simply the dot product between the vectors $a$ and $b$.


## References:
1. Caltech machine learning lectures (CS 156).
2. Article on sequential minimal optimisation: https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/
3. Various resources on math (Lagrangian multipliers, etc.)
   
